{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5de322b",
   "metadata": {},
   "source": [
    "#### A Quick Demo for the Direct Weight Estimation (DWE) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6653e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchmin import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2146b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the one-hidden-layer neural network model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size=1000, hidden_size=100, output_size=10):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_size, out_features=output_size, bias=True)\n",
    "    def forward(self, X):\n",
    "        FX = F.relu(self.fc1(X)) # hidden layer activation features\n",
    "        prob = F.softmax(self.fc2(FX), dim=1) # probability output\n",
    "        return FX, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa5817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the Direct Weight Estimation (DWE) algorithm as a class, following the sklearn style\n",
    "class DWE:\n",
    "    def __init__(self, input_size=1024, hidden_size=512, output_size=68, seed=1000, device=torch.device('cpu'),\n",
    "                 epoch=200, start=50, lamdaE=0.1, div='L2', epsilon=0.01, batch_size=4, lr=1e-3, log=False):\n",
    "        # in the training procedure, the total batch size per iteration is: batch_size * 2\n",
    "        args_values = locals()\n",
    "        args_values.pop(\"self\")\n",
    "        for arg,value in args_values.items():\n",
    "            setattr(self, arg, value)\n",
    "            \n",
    "    def fit(self, Xs, ys, Xt, yt):\n",
    "        class_labels = torch.unique(ys)\n",
    "        c = len(class_labels) # number of source classes\n",
    "        ws = torch.ones(len(ys)) # initialize all the source data weights to 1\n",
    "        \n",
    "        # define the neural network instance and the optimizer\n",
    "        torch.manual_seed(self.seed)\n",
    "        net = NeuralNet(input_size=self.input_size, hidden_size=self.hidden_size, output_size=self.output_size).to(self.device)\n",
    "        optimizer = optim.SGD(params=net.parameters(), lr=self.lr, momentum=0.9)\n",
    "        \n",
    "        #=============train the PDA network==========================\n",
    "        print('training the PDA network...')\n",
    "        m_batch = self.batch_size\n",
    "        for epoch in range(self.epoch):\n",
    "            sc_loader = torch.utils.data.DataLoader(dataset=torch.cat((Xs, ys[:,None], ws[:,None]), dim=1),\n",
    "                                                                       batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "            tg_loader = torch.utils.data.DataLoader(dataset=Xt,\n",
    "                                                                        batch_size=self.batch_size, shuffle=True, drop_last=False) \n",
    "            \n",
    "            log_loss, m_log_loss, sce_loss, m_sce_loss = 0.0, 0.0, 0.0, 0.0\n",
    "            # 2 batches of identical size are drawn from the source and target datasets\n",
    "            for sc_batch, tg_batch in zip(sc_loader, tg_loader):\n",
    "                Xs_batch, ys_batch = sc_batch[:, :-2].to(self.device,torch.float32), sc_batch[:, -2].to(self.device,torch.int64)\n",
    "                ws_batch = sc_batch[:, -1].to(self.device,torch.int64)\n",
    "                Xt_batch = tg_batch.to(self.device, torch.float32)\n",
    "                X_batch = torch.cat((Xs_batch, Xt_batch), dim=0)\n",
    "                \n",
    "                prob = net(X_batch)[1]\n",
    "                negative_log = -torch.mean(ws_batch * torch.sum(torch.log(prob[:m_batch]) * F.one_hot(ys_batch, c), dim=1))      \n",
    "                sce = -torch.mean(torch.sum((prob[m_batch:] - 1.0)**2, dim=1))   # squared-loss conditional entropy\n",
    "                loss = negative_log + self.lamdaE * sce\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "                log_loss += negative_log.item() * m_batch\n",
    "                m_log_loss += m_batch\n",
    "                sce_loss += sce.item() * m_batch\n",
    "                m_sce_loss += m_batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Xt, yt = torch.as_tensor(Xt, dtype=torch.float32, device=self.device), torch.as_tensor(yt, dtype=torch.int64, device=self.device)    \n",
    "                yt_hat = torch.argmax(net(Xt)[1], dim=1)\n",
    "                correct = torch.sum((yt_hat == yt)).item()\n",
    "                m_test = len(yt)\n",
    "            \n",
    "            if ((epoch+1) >= self.start) and ((epoch+1) % 10==0):\n",
    "                print('update the source data weights after', (epoch+1), 'epochs')\n",
    "                ms, mt = len(Xs), len(Xt)\n",
    "                Xs, ys = torch.as_tensor(Xs, dtype=torch.float32, device=self.device), torch.as_tensor(ys, dtype=torch.int64, device=self.device)  \n",
    "                FXs, FXt = net(Xs)[0], net(Xt)[0]\n",
    "                FXs, FXt = FXs / torch.norm(FXs, dim=1, keepdim=True), FXt / torch.norm(FXt, dim=1, keepdim=True)\n",
    "                FX, y = torch.cat((FXs, FXt), dim=0), torch.cat((ys, yt_hat), dim=0)\n",
    "                FX_norm = torch.sum(FX ** 2, axis = -1)\n",
    "                FXs_norm = torch.sum(FXs ** 2, axis = -1)\n",
    "                \n",
    "                if self.div == 'L2':\n",
    "                    # estimate the source data weights under the L2 distance\n",
    "                    K = torch.exp(-(FX_norm[:,None] + FXs_norm[None,:] - 2 * torch.matmul(FX, FXs.t())) / (2 * 1.0 / torch.pi)) * torch.as_tensor(y[:,None]==ys, dtype=torch.float32, device=self.device) # kernel matrix  \n",
    "                    Ks, Kt = K[:ms], K[ms:]\n",
    "                    Kt_mean = torch.mean(Kt, dim=0)\n",
    "                    H = torch.exp(-(FXs_norm[:,None] + FXs_norm[None,:] - 2 * torch.matmul(FXs, FXs.t())) / (4 * 1.0 / torch.pi)) * torch.as_tensor(ys[:,None]==ys, dtype=torch.float32, device=self.device)\n",
    "                    invM = torch.inverse(H + self.epsilon * torch.eye(ms, device=self.device))\n",
    "\n",
    "                    A = (1.0 / ms)**2 * torch.matmul(torch.matmul(Ks, invM), Ks.t()) - 0.5 * (1.0 / ms)**2 * torch.matmul(torch.matmul(torch.matmul(torch.matmul(Ks, invM), H), invM), Ks.t())\n",
    "                    B1 = 2 * (1.0 / ms) * torch.matmul(torch.matmul(Kt_mean, invM), Ks.t()) \n",
    "                    B2 = (1.0 / ms) * torch.matmul(torch.matmul(torch.matmul(torch.matmul(Kt_mean, invM), H), invM), Ks.t())\n",
    "                    B = B1 - B2\n",
    "                    C = torch.matmul(torch.matmul(Kt_mean, invM), Kt_mean) - 0.5 * torch.matmul(torch.matmul(torch.matmul(torch.matmul(Kt_mean, invM), H), invM), Kt_mean)\n",
    "\n",
    "                    def obj(w):\n",
    "                        w =torch.softmax(w, dim=0) * ms\n",
    "                        div = torch.matmul(torch.matmul(w, A), w)  - torch.matmul(B, w) + C\n",
    "                        return div\n",
    "                    w0 = torch.zeros(ms, dtype=torch.float32, device=self.device)\n",
    "                    result = minimize(obj, w0, method='l-bfgs', max_iter=100) #\n",
    "                    ws = torch.softmax(result.x, dim=0) * ms\n",
    "                    \n",
    "                else: \n",
    "                    # estimate the source data weights under the Chi2 divergence\n",
    "                    pairwise_dist = torch.cdist(FX, FX, p=2)**2 \n",
    "                    sigma = torch.median(pairwise_dist[pairwise_dist!=0]) # compute the Gaussian kernel width\n",
    "\n",
    "                    K = torch.exp(-(FX_norm[:,None] + FXs_norm[None,:] - 2 * torch.matmul(FX, FXs.t())) / sigma) * torch.as_tensor(y[:,None]==ys, dtype=torch.float32, device=self.device) # kernel matrix  \n",
    "                    Ks, Kt = K[:ms], K[ms:]\n",
    "                    H = 1.0 / mt * torch.matmul(Kt.t(), Kt) \n",
    "                    invM = torch.inverse(H + self.epsilon * torch.eye(ms, device=self.device))\n",
    "                    A = 2 * (1.0 / ms)**2 * torch.matmul(torch.matmul(Ks, invM), Ks.t())\n",
    "                    B = (1.0 / ms)**2 * torch.matmul(torch.matmul(torch.matmul(torch.matmul(Ks, invM), H), invM), Ks.t())\n",
    "                    C = A - B\n",
    "\n",
    "                    def obj(w):\n",
    "                        w = torch.softmax(w, dim=0) * ms\n",
    "                        div = torch.matmul(torch.matmul(w, C), w)  - 1.0 \n",
    "                        return div\n",
    "                    w0 = torch.zeros(ms, dtype=torch.float32, device=self.device)\n",
    "                    result = minimize(obj, w0, method='l-bfgs', max_iter=100) #\n",
    "                    ws = torch.softmax(result.x, dim=0) * ms\n",
    "\n",
    "            if True == self.log:\n",
    "                print('epoch ', (epoch+1), ', log loss ',  \"{:.5f}\".format(log_loss / m_log_loss), \n",
    "                      ', sce loss ', \"{:.5f}\".format(sce_loss / m_sce_loss), \n",
    "                      ', total loss ', \"{:.5f}\".format(log_loss / m_log_loss + self.lamdaE * sce_loss / m_sce_loss),\n",
    "                      ', test acc. ', \"{:.5f}\".format((correct / m_test) * 100))  \n",
    "        #========================================================             \n",
    "        self.net = net # save the network\n",
    "\n",
    "    def score(self, Xt, yt):\n",
    "        with torch.no_grad():\n",
    "            Xt, yt = torch.as_tensor(Xt, dtype=torch.float32,device=self.device), torch.as_tensor(yt, dtype=torch.int64,device=self.device)    \n",
    "            pred = torch.argmax(self.net(Xt)[1],dim=1)\n",
    "            correct = torch.sum((pred == yt)).item()\n",
    "            m_test = len(yt)\n",
    "        return (correct / m_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0d3bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import numpy.linalg as la\n",
    "from sklearn.preprocessing import scale,LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d6343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0') #'cpu'\n",
    "data = np.loadtxt('OfficeHome/Resnet50_Art_Art.csv', delimiter=',') # source domain\n",
    "Xs, ys = torch.tensor(data[:,:-1]), data[:,-1]\n",
    "ys = torch.tensor(LabelEncoder().fit(ys).transform(ys).astype(np.float64),dtype=torch.int64)\n",
    "    \n",
    "data = np.loadtxt('OfficeHome/Resnet50_Art_Product.csv', delimiter=',') # target domain\n",
    "Xt, yt = torch.tensor(data[:,:-1]), data[:,-1]\n",
    "yt = torch.tensor(LabelEncoder().fit(yt).transform(yt).astype(np.float64),dtype=torch.int64)\n",
    "Xt, yt = Xt[yt < 25], yt[yt < 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d58f930",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the PDA network...\n",
      "epoch  1 , log loss  4.10115 , sce loss  -63.01585 , total loss  -2.20044 , test acc.  11.09244\n",
      "epoch  2 , log loss  3.69547 , sce loss  -63.01852 , total loss  -2.60638 , test acc.  18.82353\n",
      "epoch  3 , log loss  3.17982 , sce loss  -63.02477 , total loss  -3.12266 , test acc.  34.34174\n",
      "epoch  4 , log loss  2.54052 , sce loss  -63.04595 , total loss  -3.76408 , test acc.  37.70308\n",
      "epoch  5 , log loss  1.92830 , sce loss  -63.09247 , total loss  -4.38095 , test acc.  52.49300\n",
      "epoch  6 , log loss  1.47785 , sce loss  -63.15549 , total loss  -4.83770 , test acc.  59.60784\n",
      "epoch  7 , log loss  1.15947 , sce loss  -63.21565 , total loss  -5.16209 , test acc.  59.83193\n",
      "epoch  8 , log loss  0.93590 , sce loss  -63.28697 , total loss  -5.39280 , test acc.  64.03361\n",
      "epoch  9 , log loss  0.77394 , sce loss  -63.34939 , total loss  -5.56100 , test acc.  64.76190\n",
      "epoch  10 , log loss  0.69634 , sce loss  -63.40007 , total loss  -5.64367 , test acc.  65.21008\n",
      "epoch  11 , log loss  0.58636 , sce loss  -63.43289 , total loss  -5.75692 , test acc.  68.51541\n",
      "epoch  12 , log loss  0.55824 , sce loss  -63.48007 , total loss  -5.78977 , test acc.  70.70028\n",
      "epoch  13 , log loss  0.49536 , sce loss  -63.48885 , total loss  -5.85352 , test acc.  69.97199\n",
      "epoch  14 , log loss  0.44435 , sce loss  -63.51999 , total loss  -5.90765 , test acc.  70.53221\n",
      "epoch  15 , log loss  0.39268 , sce loss  -63.54383 , total loss  -5.96170 , test acc.  71.48459\n",
      "epoch  16 , log loss  0.39415 , sce loss  -63.56113 , total loss  -5.96196 , test acc.  73.10924\n",
      "epoch  17 , log loss  0.36809 , sce loss  -63.58066 , total loss  -5.98998 , test acc.  70.92437\n",
      "epoch  18 , log loss  0.33981 , sce loss  -63.59463 , total loss  -6.01965 , test acc.  73.44538\n",
      "epoch  19 , log loss  0.31597 , sce loss  -63.61339 , total loss  -6.04537 , test acc.  73.66947\n",
      "epoch  20 , log loss  0.30602 , sce loss  -63.62435 , total loss  -6.05641 , test acc.  73.44538\n",
      "epoch  21 , log loss  0.27868 , sce loss  -63.63734 , total loss  -6.08505 , test acc.  74.62185\n",
      "epoch  22 , log loss  0.26613 , sce loss  -63.64804 , total loss  -6.09868 , test acc.  74.78992\n",
      "epoch  23 , log loss  0.25672 , sce loss  -63.65830 , total loss  -6.10911 , test acc.  74.28571\n",
      "epoch  24 , log loss  0.24009 , sce loss  -63.66954 , total loss  -6.12687 , test acc.  75.07003\n",
      "epoch  25 , log loss  0.23400 , sce loss  -63.67769 , total loss  -6.13376 , test acc.  73.50140\n",
      "epoch  26 , log loss  0.21734 , sce loss  -63.68311 , total loss  -6.15097 , test acc.  75.68627\n",
      "epoch  27 , log loss  0.19827 , sce loss  -63.69140 , total loss  -6.17087 , test acc.  74.90196\n",
      "epoch  28 , log loss  0.19074 , sce loss  -63.69721 , total loss  -6.17898 , test acc.  75.18207\n",
      "epoch  29 , log loss  0.18511 , sce loss  -63.70857 , total loss  -6.18574 , test acc.  76.24650\n",
      "epoch  30 , log loss  0.18111 , sce loss  -63.71433 , total loss  -6.19032 , test acc.  75.40616\n",
      "epoch  31 , log loss  0.17218 , sce loss  -63.71727 , total loss  -6.19955 , test acc.  75.51821\n",
      "epoch  32 , log loss  0.16280 , sce loss  -63.72467 , total loss  -6.20966 , test acc.  76.41457\n",
      "epoch  33 , log loss  0.16425 , sce loss  -63.73204 , total loss  -6.20895 , test acc.  76.41457\n",
      "epoch  34 , log loss  0.14556 , sce loss  -63.73664 , total loss  -6.22811 , test acc.  75.35014\n",
      "epoch  35 , log loss  0.14972 , sce loss  -63.73880 , total loss  -6.22416 , test acc.  76.13445\n",
      "epoch  36 , log loss  0.13783 , sce loss  -63.74582 , total loss  -6.23675 , test acc.  75.79832\n",
      "epoch  37 , log loss  0.13292 , sce loss  -63.74979 , total loss  -6.24206 , test acc.  76.35854\n",
      "epoch  38 , log loss  0.12463 , sce loss  -63.75065 , total loss  -6.25044 , test acc.  75.91036\n",
      "epoch  39 , log loss  0.12399 , sce loss  -63.75571 , total loss  -6.25158 , test acc.  75.91036\n",
      "epoch  40 , log loss  0.11844 , sce loss  -63.76006 , total loss  -6.25757 , test acc.  75.68627\n",
      "epoch  41 , log loss  0.12106 , sce loss  -63.76309 , total loss  -6.25525 , test acc.  76.58263\n",
      "epoch  42 , log loss  0.10800 , sce loss  -63.76922 , total loss  -6.26892 , test acc.  76.35854\n",
      "epoch  43 , log loss  0.11145 , sce loss  -63.77118 , total loss  -6.26567 , test acc.  76.80672\n",
      "epoch  44 , log loss  0.10263 , sce loss  -63.77371 , total loss  -6.27474 , test acc.  76.86275\n",
      "epoch  45 , log loss  0.10032 , sce loss  -63.77546 , total loss  -6.27723 , test acc.  76.30252\n",
      "epoch  46 , log loss  0.10296 , sce loss  -63.77973 , total loss  -6.27501 , test acc.  76.52661\n",
      "epoch  47 , log loss  0.09373 , sce loss  -63.78378 , total loss  -6.28465 , test acc.  76.91877\n",
      "epoch  48 , log loss  0.09492 , sce loss  -63.78706 , total loss  -6.28379 , test acc.  76.58263\n",
      "epoch  49 , log loss  0.08956 , sce loss  -63.78734 , total loss  -6.28918 , test acc.  76.69468\n",
      "update the source data weights after 50 epochs\n",
      "epoch  50 , log loss  0.08887 , sce loss  -63.79133 , total loss  -6.29026 , test acc.  76.35854\n",
      "epoch  51 , log loss  0.03604 , sce loss  -63.80103 , total loss  -6.34407 , test acc.  78.09524\n",
      "epoch  52 , log loss  0.03184 , sce loss  -63.82695 , total loss  -6.35085 , test acc.  77.98319\n",
      "epoch  53 , log loss  0.02003 , sce loss  -63.84293 , total loss  -6.36426 , test acc.  80.05602\n",
      "epoch  54 , log loss  0.02396 , sce loss  -63.85106 , total loss  -6.36114 , test acc.  77.81513\n",
      "epoch  55 , log loss  0.06559 , sce loss  -63.85458 , total loss  -6.31987 , test acc.  81.12045\n",
      "epoch  56 , log loss  0.04155 , sce loss  -63.86019 , total loss  -6.34447 , test acc.  78.82353\n",
      "epoch  57 , log loss  0.02382 , sce loss  -63.86779 , total loss  -6.36296 , test acc.  80.50420\n",
      "epoch  58 , log loss  0.10395 , sce loss  -63.86239 , total loss  -6.28229 , test acc.  79.55182\n",
      "epoch  59 , log loss  0.08205 , sce loss  -63.86727 , total loss  -6.30467 , test acc.  80.22409\n",
      "update the source data weights after 60 epochs\n",
      "epoch  60 , log loss  0.01042 , sce loss  -63.86870 , total loss  -6.37645 , test acc.  80.16807\n",
      "epoch  61 , log loss  0.01051 , sce loss  -63.86331 , total loss  -6.37582 , test acc.  81.12045\n",
      "epoch  62 , log loss  0.00786 , sce loss  -63.87234 , total loss  -6.37938 , test acc.  81.40056\n",
      "epoch  63 , log loss  0.00648 , sce loss  -63.87399 , total loss  -6.38092 , test acc.  81.28852\n",
      "epoch  64 , log loss  0.00514 , sce loss  -63.87446 , total loss  -6.38231 , test acc.  81.56863\n",
      "epoch  65 , log loss  0.00354 , sce loss  -63.87763 , total loss  -6.38422 , test acc.  82.12885\n",
      "epoch  66 , log loss  0.00469 , sce loss  -63.88099 , total loss  -6.38341 , test acc.  82.46499\n",
      "epoch  67 , log loss  0.00377 , sce loss  -63.88293 , total loss  -6.38452 , test acc.  82.24090\n",
      "epoch  68 , log loss  0.00342 , sce loss  -63.88542 , total loss  -6.38512 , test acc.  82.52101\n",
      "epoch  69 , log loss  0.00299 , sce loss  -63.88786 , total loss  -6.38580 , test acc.  82.80112\n",
      "update the source data weights after 70 epochs\n",
      "epoch  70 , log loss  0.00351 , sce loss  -63.88981 , total loss  -6.38547 , test acc.  82.91317\n",
      "epoch  71 , log loss  0.00309 , sce loss  -63.89214 , total loss  -6.38612 , test acc.  82.80112\n",
      "epoch  72 , log loss  0.00249 , sce loss  -63.89506 , total loss  -6.38701 , test acc.  83.08123\n",
      "epoch  73 , log loss  0.00315 , sce loss  -63.89741 , total loss  -6.38659 , test acc.  83.08123\n",
      "epoch  74 , log loss  0.00278 , sce loss  -63.89862 , total loss  -6.38708 , test acc.  83.30532\n",
      "epoch  75 , log loss  0.00279 , sce loss  -63.89887 , total loss  -6.38710 , test acc.  83.19328\n",
      "epoch  76 , log loss  0.00278 , sce loss  -63.90069 , total loss  -6.38729 , test acc.  83.52941\n",
      "epoch  77 , log loss  0.00285 , sce loss  -63.90292 , total loss  -6.38744 , test acc.  83.30532\n",
      "epoch  78 , log loss  0.00264 , sce loss  -63.90416 , total loss  -6.38778 , test acc.  83.58543\n",
      "epoch  79 , log loss  0.00245 , sce loss  -63.90508 , total loss  -6.38806 , test acc.  83.75350\n",
      "update the source data weights after 80 epochs\n",
      "epoch  80 , log loss  0.00256 , sce loss  -63.90635 , total loss  -6.38808 , test acc.  83.75350\n",
      "epoch  81 , log loss  0.00208 , sce loss  -63.90776 , total loss  -6.38870 , test acc.  83.69748\n",
      "epoch  82 , log loss  0.00215 , sce loss  -63.90934 , total loss  -6.38878 , test acc.  83.75350\n",
      "epoch  83 , log loss  0.00235 , sce loss  -63.91096 , total loss  -6.38875 , test acc.  83.52941\n",
      "epoch  84 , log loss  0.00193 , sce loss  -63.91201 , total loss  -6.38927 , test acc.  83.58543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  85 , log loss  0.00179 , sce loss  -63.91315 , total loss  -6.38952 , test acc.  83.58543\n",
      "epoch  86 , log loss  0.00218 , sce loss  -63.91411 , total loss  -6.38923 , test acc.  83.41737\n",
      "epoch  87 , log loss  0.00184 , sce loss  -63.91522 , total loss  -6.38968 , test acc.  83.58543\n",
      "epoch  88 , log loss  0.00201 , sce loss  -63.91584 , total loss  -6.38957 , test acc.  83.52941\n",
      "epoch  89 , log loss  0.00188 , sce loss  -63.91677 , total loss  -6.38979 , test acc.  83.47339\n",
      "update the source data weights after 90 epochs\n",
      "epoch  90 , log loss  0.00166 , sce loss  -63.91777 , total loss  -6.39012 , test acc.  83.52941\n",
      "epoch  91 , log loss  0.00162 , sce loss  -63.91890 , total loss  -6.39027 , test acc.  83.58543\n",
      "epoch  92 , log loss  0.00191 , sce loss  -63.92008 , total loss  -6.39010 , test acc.  83.64146\n",
      "epoch  93 , log loss  0.00174 , sce loss  -63.92096 , total loss  -6.39035 , test acc.  83.64146\n",
      "epoch  94 , log loss  0.00188 , sce loss  -63.92202 , total loss  -6.39032 , test acc.  83.69748\n",
      "epoch  95 , log loss  0.00173 , sce loss  -63.92265 , total loss  -6.39053 , test acc.  83.69748\n",
      "epoch  96 , log loss  0.00144 , sce loss  -63.92366 , total loss  -6.39093 , test acc.  83.58543\n",
      "epoch  97 , log loss  0.00168 , sce loss  -63.92426 , total loss  -6.39074 , test acc.  83.58543\n",
      "epoch  98 , log loss  0.00150 , sce loss  -63.92500 , total loss  -6.39100 , test acc.  83.47339\n",
      "epoch  99 , log loss  0.00144 , sce loss  -63.92553 , total loss  -6.39112 , test acc.  83.75350\n",
      "update the source data weights after 100 epochs\n",
      "epoch  100 , log loss  0.00158 , sce loss  -63.92629 , total loss  -6.39105 , test acc.  83.69748\n",
      "epoch  101 , log loss  0.00166 , sce loss  -63.92684 , total loss  -6.39102 , test acc.  83.69748\n",
      "epoch  102 , log loss  0.00174 , sce loss  -63.92739 , total loss  -6.39100 , test acc.  83.69748\n",
      "epoch  103 , log loss  0.00175 , sce loss  -63.92796 , total loss  -6.39104 , test acc.  83.64146\n",
      "epoch  104 , log loss  0.00144 , sce loss  -63.92846 , total loss  -6.39140 , test acc.  83.69748\n",
      "epoch  105 , log loss  0.00158 , sce loss  -63.92893 , total loss  -6.39131 , test acc.  83.80952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83.80952380952381"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = DWE(input_size=2048, hidden_size=1024, output_size=65, seed=0, device=DEVICE,\n",
    "                         epoch=105, start=50, lamdaE=0.1, div='L2', epsilon=1e-2, batch_size=200, lr=1e-2, log=True)\n",
    "instance.fit(Xs, ys, Xt, yt)\n",
    "instance.score(Xt, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5659cf80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the PDA network...\n",
      "epoch  1 , log loss  4.10115 , sce loss  -63.01585 , total loss  -2.20044 , test acc.  11.09244\n",
      "epoch  2 , log loss  3.69547 , sce loss  -63.01852 , total loss  -2.60638 , test acc.  18.82353\n",
      "epoch  3 , log loss  3.17982 , sce loss  -63.02477 , total loss  -3.12266 , test acc.  34.34174\n",
      "epoch  4 , log loss  2.54052 , sce loss  -63.04595 , total loss  -3.76408 , test acc.  37.70308\n",
      "epoch  5 , log loss  1.92830 , sce loss  -63.09247 , total loss  -4.38095 , test acc.  52.49300\n",
      "epoch  6 , log loss  1.47785 , sce loss  -63.15549 , total loss  -4.83770 , test acc.  59.60784\n",
      "epoch  7 , log loss  1.15947 , sce loss  -63.21565 , total loss  -5.16209 , test acc.  59.83193\n",
      "epoch  8 , log loss  0.93590 , sce loss  -63.28697 , total loss  -5.39280 , test acc.  64.03361\n",
      "epoch  9 , log loss  0.77394 , sce loss  -63.34939 , total loss  -5.56100 , test acc.  64.76190\n",
      "epoch  10 , log loss  0.69634 , sce loss  -63.40007 , total loss  -5.64367 , test acc.  65.21008\n",
      "epoch  11 , log loss  0.58636 , sce loss  -63.43289 , total loss  -5.75692 , test acc.  68.51541\n",
      "epoch  12 , log loss  0.55824 , sce loss  -63.48007 , total loss  -5.78977 , test acc.  70.70028\n",
      "epoch  13 , log loss  0.49536 , sce loss  -63.48885 , total loss  -5.85352 , test acc.  69.97199\n",
      "epoch  14 , log loss  0.44435 , sce loss  -63.51999 , total loss  -5.90765 , test acc.  70.53221\n",
      "epoch  15 , log loss  0.39268 , sce loss  -63.54383 , total loss  -5.96170 , test acc.  71.48459\n",
      "epoch  16 , log loss  0.39415 , sce loss  -63.56113 , total loss  -5.96196 , test acc.  73.10924\n",
      "epoch  17 , log loss  0.36809 , sce loss  -63.58066 , total loss  -5.98998 , test acc.  70.92437\n",
      "epoch  18 , log loss  0.33981 , sce loss  -63.59463 , total loss  -6.01965 , test acc.  73.44538\n",
      "epoch  19 , log loss  0.31597 , sce loss  -63.61339 , total loss  -6.04537 , test acc.  73.66947\n",
      "epoch  20 , log loss  0.30602 , sce loss  -63.62435 , total loss  -6.05641 , test acc.  73.44538\n",
      "epoch  21 , log loss  0.27868 , sce loss  -63.63734 , total loss  -6.08505 , test acc.  74.62185\n",
      "epoch  22 , log loss  0.26613 , sce loss  -63.64804 , total loss  -6.09868 , test acc.  74.78992\n",
      "epoch  23 , log loss  0.25672 , sce loss  -63.65830 , total loss  -6.10911 , test acc.  74.28571\n",
      "epoch  24 , log loss  0.24009 , sce loss  -63.66954 , total loss  -6.12687 , test acc.  75.07003\n",
      "epoch  25 , log loss  0.23400 , sce loss  -63.67769 , total loss  -6.13376 , test acc.  73.50140\n",
      "epoch  26 , log loss  0.21734 , sce loss  -63.68311 , total loss  -6.15097 , test acc.  75.68627\n",
      "epoch  27 , log loss  0.19827 , sce loss  -63.69140 , total loss  -6.17087 , test acc.  74.90196\n",
      "epoch  28 , log loss  0.19074 , sce loss  -63.69721 , total loss  -6.17898 , test acc.  75.18207\n",
      "epoch  29 , log loss  0.18511 , sce loss  -63.70857 , total loss  -6.18574 , test acc.  76.24650\n",
      "epoch  30 , log loss  0.18111 , sce loss  -63.71433 , total loss  -6.19032 , test acc.  75.40616\n",
      "epoch  31 , log loss  0.17218 , sce loss  -63.71727 , total loss  -6.19955 , test acc.  75.51821\n",
      "epoch  32 , log loss  0.16280 , sce loss  -63.72467 , total loss  -6.20966 , test acc.  76.41457\n",
      "epoch  33 , log loss  0.16425 , sce loss  -63.73204 , total loss  -6.20895 , test acc.  76.41457\n",
      "epoch  34 , log loss  0.14556 , sce loss  -63.73664 , total loss  -6.22811 , test acc.  75.35014\n",
      "epoch  35 , log loss  0.14972 , sce loss  -63.73880 , total loss  -6.22416 , test acc.  76.13445\n",
      "epoch  36 , log loss  0.13783 , sce loss  -63.74582 , total loss  -6.23675 , test acc.  75.79832\n",
      "epoch  37 , log loss  0.13292 , sce loss  -63.74979 , total loss  -6.24206 , test acc.  76.35854\n",
      "epoch  38 , log loss  0.12463 , sce loss  -63.75065 , total loss  -6.25044 , test acc.  75.91036\n",
      "epoch  39 , log loss  0.12399 , sce loss  -63.75571 , total loss  -6.25158 , test acc.  75.91036\n",
      "epoch  40 , log loss  0.11844 , sce loss  -63.76006 , total loss  -6.25757 , test acc.  75.68627\n",
      "epoch  41 , log loss  0.12106 , sce loss  -63.76309 , total loss  -6.25525 , test acc.  76.58263\n",
      "epoch  42 , log loss  0.10800 , sce loss  -63.76922 , total loss  -6.26892 , test acc.  76.35854\n",
      "epoch  43 , log loss  0.11145 , sce loss  -63.77118 , total loss  -6.26567 , test acc.  76.80672\n",
      "epoch  44 , log loss  0.10263 , sce loss  -63.77371 , total loss  -6.27474 , test acc.  76.86275\n",
      "epoch  45 , log loss  0.10032 , sce loss  -63.77546 , total loss  -6.27723 , test acc.  76.30252\n",
      "epoch  46 , log loss  0.10296 , sce loss  -63.77973 , total loss  -6.27501 , test acc.  76.52661\n",
      "epoch  47 , log loss  0.09373 , sce loss  -63.78378 , total loss  -6.28465 , test acc.  76.91877\n",
      "epoch  48 , log loss  0.09492 , sce loss  -63.78706 , total loss  -6.28379 , test acc.  76.58263\n",
      "epoch  49 , log loss  0.08956 , sce loss  -63.78734 , total loss  -6.28918 , test acc.  76.69468\n",
      "update the source data weights after 50 epochs\n",
      "epoch  50 , log loss  0.08887 , sce loss  -63.79133 , total loss  -6.29026 , test acc.  76.35854\n",
      "epoch  51 , log loss  0.12644 , sce loss  -63.80081 , total loss  -6.25364 , test acc.  78.76751\n",
      "epoch  52 , log loss  0.10558 , sce loss  -63.82316 , total loss  -6.27674 , test acc.  77.36695\n",
      "epoch  53 , log loss  0.08038 , sce loss  -63.83593 , total loss  -6.30321 , test acc.  79.38375\n",
      "epoch  54 , log loss  0.06423 , sce loss  -63.83703 , total loss  -6.31947 , test acc.  78.31933\n",
      "epoch  55 , log loss  0.06613 , sce loss  -63.83240 , total loss  -6.31711 , test acc.  78.59944\n",
      "epoch  56 , log loss  0.05579 , sce loss  -63.83341 , total loss  -6.32755 , test acc.  78.15126\n",
      "epoch  57 , log loss  0.03316 , sce loss  -63.84284 , total loss  -6.35112 , test acc.  77.42297\n",
      "epoch  58 , log loss  0.03161 , sce loss  -63.84442 , total loss  -6.35283 , test acc.  79.43978\n",
      "epoch  59 , log loss  0.02408 , sce loss  -63.84693 , total loss  -6.36061 , test acc.  79.15966\n",
      "update the source data weights after 60 epochs\n",
      "epoch  60 , log loss  0.02358 , sce loss  -63.85128 , total loss  -6.36155 , test acc.  78.71148\n",
      "epoch  61 , log loss  0.02127 , sce loss  -63.85317 , total loss  -6.36404 , test acc.  79.04762\n",
      "epoch  62 , log loss  0.01786 , sce loss  -63.85584 , total loss  -6.36773 , test acc.  78.93557\n",
      "epoch  63 , log loss  0.01774 , sce loss  -63.85831 , total loss  -6.36809 , test acc.  79.04762\n",
      "epoch  64 , log loss  0.01518 , sce loss  -63.86056 , total loss  -6.37087 , test acc.  79.15966\n",
      "epoch  65 , log loss  0.01303 , sce loss  -63.86374 , total loss  -6.37335 , test acc.  78.93557\n",
      "epoch  66 , log loss  0.01282 , sce loss  -63.86649 , total loss  -6.37383 , test acc.  79.38375\n",
      "epoch  67 , log loss  0.01198 , sce loss  -63.86869 , total loss  -6.37489 , test acc.  79.71989\n",
      "epoch  68 , log loss  0.01131 , sce loss  -63.87032 , total loss  -6.37572 , test acc.  79.77591\n",
      "epoch  69 , log loss  0.01142 , sce loss  -63.87097 , total loss  -6.37568 , test acc.  79.88796\n",
      "update the source data weights after 70 epochs\n",
      "epoch  70 , log loss  0.01076 , sce loss  -63.87156 , total loss  -6.37640 , test acc.  79.55182\n",
      "epoch  71 , log loss  0.01069 , sce loss  -63.87371 , total loss  -6.37669 , test acc.  79.43978\n",
      "epoch  72 , log loss  0.00832 , sce loss  -63.87573 , total loss  -6.37925 , test acc.  79.77591\n",
      "epoch  73 , log loss  0.00888 , sce loss  -63.87699 , total loss  -6.37882 , test acc.  79.77591\n",
      "epoch  74 , log loss  0.00919 , sce loss  -63.87862 , total loss  -6.37867 , test acc.  79.83193\n",
      "epoch  75 , log loss  0.00810 , sce loss  -63.87923 , total loss  -6.37982 , test acc.  79.77591\n",
      "epoch  76 , log loss  0.00797 , sce loss  -63.88102 , total loss  -6.38013 , test acc.  79.66387\n",
      "epoch  77 , log loss  0.00825 , sce loss  -63.88291 , total loss  -6.38004 , test acc.  79.83193\n",
      "epoch  78 , log loss  0.00748 , sce loss  -63.88381 , total loss  -6.38090 , test acc.  80.16807\n",
      "epoch  79 , log loss  0.00712 , sce loss  -63.88446 , total loss  -6.38132 , test acc.  79.94398\n",
      "update the source data weights after 80 epochs\n",
      "epoch  80 , log loss  0.00721 , sce loss  -63.88590 , total loss  -6.38138 , test acc.  80.11204\n",
      "epoch  81 , log loss  0.00604 , sce loss  -63.88800 , total loss  -6.38276 , test acc.  80.11204\n",
      "epoch  82 , log loss  0.00602 , sce loss  -63.88968 , total loss  -6.38295 , test acc.  80.16807\n",
      "epoch  83 , log loss  0.00607 , sce loss  -63.89145 , total loss  -6.38308 , test acc.  80.11204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  84 , log loss  0.00561 , sce loss  -63.89235 , total loss  -6.38362 , test acc.  80.11204\n",
      "epoch  85 , log loss  0.00510 , sce loss  -63.89330 , total loss  -6.38423 , test acc.  80.05602\n",
      "epoch  86 , log loss  0.00544 , sce loss  -63.89414 , total loss  -6.38397 , test acc.  80.05602\n",
      "epoch  87 , log loss  0.00525 , sce loss  -63.89596 , total loss  -6.38435 , test acc.  80.05602\n",
      "epoch  88 , log loss  0.00485 , sce loss  -63.89680 , total loss  -6.38483 , test acc.  80.00000\n",
      "epoch  89 , log loss  0.00563 , sce loss  -63.89799 , total loss  -6.38417 , test acc.  80.05602\n",
      "update the source data weights after 90 epochs\n",
      "epoch  90 , log loss  0.00484 , sce loss  -63.89923 , total loss  -6.38508 , test acc.  80.05602\n",
      "epoch  91 , log loss  0.00451 , sce loss  -63.90117 , total loss  -6.38561 , test acc.  80.05602\n",
      "epoch  92 , log loss  0.00469 , sce loss  -63.90280 , total loss  -6.38559 , test acc.  79.94398\n",
      "epoch  93 , log loss  0.00406 , sce loss  -63.90346 , total loss  -6.38628 , test acc.  80.05602\n",
      "epoch  94 , log loss  0.00464 , sce loss  -63.90463 , total loss  -6.38582 , test acc.  80.05602\n",
      "epoch  95 , log loss  0.00415 , sce loss  -63.90569 , total loss  -6.38642 , test acc.  80.05602\n",
      "epoch  96 , log loss  0.00401 , sce loss  -63.90697 , total loss  -6.38668 , test acc.  80.16807\n",
      "epoch  97 , log loss  0.00410 , sce loss  -63.90786 , total loss  -6.38668 , test acc.  80.22409\n",
      "epoch  98 , log loss  0.00400 , sce loss  -63.90842 , total loss  -6.38684 , test acc.  80.22409\n",
      "epoch  99 , log loss  0.00351 , sce loss  -63.90873 , total loss  -6.38736 , test acc.  80.16807\n",
      "update the source data weights after 100 epochs\n",
      "epoch  100 , log loss  0.00362 , sce loss  -63.90968 , total loss  -6.38734 , test acc.  80.11204\n",
      "epoch  101 , log loss  0.00389 , sce loss  -63.91087 , total loss  -6.38720 , test acc.  80.11204\n",
      "epoch  102 , log loss  0.00428 , sce loss  -63.91149 , total loss  -6.38687 , test acc.  80.11204\n",
      "epoch  103 , log loss  0.00422 , sce loss  -63.91211 , total loss  -6.38699 , test acc.  80.22409\n",
      "epoch  104 , log loss  0.00367 , sce loss  -63.91277 , total loss  -6.38761 , test acc.  80.44818\n",
      "epoch  105 , log loss  0.00368 , sce loss  -63.91293 , total loss  -6.38761 , test acc.  80.39216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80.3921568627451"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = DWE(input_size=2048, hidden_size=1024, output_size=65, seed=0, device=DEVICE,\n",
    "                         epoch=105, start=50, lamdaE=0.1, div='Chi2', epsilon=1e-2, batch_size=200, lr=1e-2, log=True)\n",
    "instance.fit(Xs, ys, Xt, yt)\n",
    "instance.score(Xt, yt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
